{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxdkXlL6q3D/GcLvLFJSml",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robrocha42/Desafio_Titanic_Kaggle/blob/main/Desafio_Titanic_Kaggle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Desafio Titanic - Site Kaggle - Orientado a objetos"
      ],
      "metadata": {
        "id": "s2XExUDlTbhw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8DlFCwFRJUn"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "class DataPrep:\n",
        " def __init__(self, data: pd.DataFrame) -> None:\n",
        "  \"Inicializa a classe DataPrep com a base de dados do Titanic.\"\n",
        "  self.data = data\n",
        "\n",
        " def tratar_nulos(self) -> None:\n",
        "  \"Faz o tratamento das variáveis nulas, imputando o valor adequado.\"\n",
        "  # Imputar mediana das idades por classe e sexo\n",
        "  self.data['Age'] = self.data.groupby(['Pclass', 'Sex'])['Age'] \\\n",
        "  .apply(lambda x: x.fillna(x.median()))\n",
        "  # Imputar local de embarque\n",
        "  self.data['Embarked'] = self.data['Embarked'].fillna('S')\n",
        " \n",
        " def tratar_variaveis_categoricas(self) -> None:\n",
        "  \"Faz o tratamento das variáveis categóricas\"\n",
        "  # Label Encoding da variável Sex\n",
        "  sexo = {'male': 0, 'female': 1}\n",
        "  self.data['Sex'] = self.data['Sex'].map(sexo)\n",
        "  # One Hot Encoding da variável Embarked\n",
        "  embarked_dummies = pd.get_dummies(self.data['Embarked'])\n",
        "  self.data = pd.concat([self.data, embarked_dummies], axis=1)\n",
        "\n",
        " def criar_variaveis(self) -> None:\n",
        "  # Vamos somar a quantidade de irmãos e cônjuges,\n",
        "  # a quantidade de pais e filhos,\n",
        "  # e mais 1 para considera o próprio passageiro.\n",
        "  self.data['FamilySize'] = self.data['SibSp'] + self.data['Parch'] + 1\n",
        "\n",
        " def remover_variaveis(self) -> None:\n",
        "  \"Remove as variáveis que não serão utilizadas pelo modelo para melhorar o desempenho\"\n",
        "  colunas_para_remover = [\n",
        "  'PassengerId',\n",
        "  'Name',\n",
        "  'Ticket',\n",
        "  'Cabin', # Variável com muitos dados faltantes.\n",
        "  'Embarked', # Foram criadas variáveis dummies.\n",
        "  'SibSp', 'Parch' # Foram combinadas em uma nova variável.\n",
        "  ]\n",
        "  self.data.drop(columns=colunas_para_remover, inplace=True)\n",
        "\n",
        " def normalizar_dados(self) -> None:\n",
        "  variaveis = self.data.drop(columns='Survived')\n",
        "  var_cols = variaveis.columns\n",
        "  resposta = self.data['Survived']\n",
        "  scaler = MinMaxScaler()\n",
        "  variaveis = scaler.fit_transform(variaveis)\n",
        "  variaveis = pd.DataFrame(variaveis, columns=var_cols)\n",
        " \n",
        "  self.data = pd.concat([variaveis, resposta], axis=1)\n",
        "\n",
        " def separar_treino_teste(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "  \"Separa a base de dados entre conjunto de treinamento e teste.\"\n",
        "  treino, teste = train_test_split(self.data, test_size=0.3, random_state=2023)\n",
        "  return treino, teste\n",
        "\n",
        " def preparar_dados(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "  \"Executa todas as etapas de transformação de dados.\"\n",
        "  self.tratar_nulos()\n",
        "  self.tratar_variaveis_categoricas()\n",
        "  self.criar_variaveis()\n",
        "  self.remover_variaveis()\n",
        "  self.normalizar_dados()\n",
        "  treino, teste = self.separar_treino_teste()\n",
        "  return treino, teste"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparando os dados para a modelagem\n",
        "\n",
        "df = pd.read_csv('/content/train.csv') #Caminho do arquivo\n",
        "dp = DataPrep(df)\n",
        "# Toda a preparação de dados necessaria e retorna os DataFrames de treino e teste já separados\n",
        "df_treino, df_teste = dp.preparar_dados()\n",
        "\n",
        "#Treinando o modelo com os conjuntos de dados.\n",
        "#Separando as variáveis preditoras da variável resposta para \n",
        "#ambos DataFrames - treino e teste\n",
        "X_treino = df_treino.drop(columns='Survived')\n",
        "Y_treino = df_treino['Survived']\n",
        "\n",
        "X_teste = df_teste.drop(columns='Survived')\n",
        "Y_teste = df_teste['Survived']\n",
        "\n",
        "#Instanciando o objeto\n",
        "clf = LogisticRegression()\n",
        "\n",
        "#Teinando o modelo utilizando os dados de treinamento.\n",
        "clf.fit(X_treino, Y_treino)\n",
        "#Avaliando a performance do modelo utilizando o conjunto de dados de teste \n",
        "#78% de acurácia com os dados tratados\n",
        "print(\"Acurácia do modelo: \", clf.score(X_teste, Y_teste),\"\\n\\n\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Em projetos de machine learning é comum termos muitas possíveis variáveis preditoras \n",
        "ou criarmos variáveis que não têm um grande poder preditivo. Ambos os casos devem ser evitados. \n",
        "Ter muitas variáveis preditoras irá aumentar a complexidade do modelo, em muitos casos desnecessariamente, \n",
        "e pode fazer com que o algoritmo treinado tenha \n",
        "dificuldade de fazer previsões apuradas para novos dados. O mesmo pode acontecer quando temos variáveis com baixo poder preditivo.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "86_cTvxESQDF",
        "outputId": "b925e648-d26b-4b15-80cf-9a05a02dc96e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia do modelo:  0.7835820895522388 \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-258844417806>:16: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
            "To preserve the previous behavior, use\n",
            "\n",
            "\t>>> .groupby(..., group_keys=False)\n",
            "\n",
            "To adopt the future behavior and silence this warning, use \n",
            "\n",
            "\t>>> .groupby(..., group_keys=True)\n",
            "  .apply(lambda x: x.fillna(x.median()))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEm projetos de machine learning é comum termos muitas possíveis variáveis preditoras \\nou criarmos variáveis que não têm um grande poder preditivo. Ambos os casos devem ser evitados. \\nTer muitas variáveis preditoras irá aumentar a complexidade do modelo, em muitos casos desnecessariamente, \\ne pode fazer com que o algoritmo treinado tenha \\ndificuldade de fazer previsões apuradas para novos dados. O mesmo pode acontecer quando temos variáveis com baixo poder preditivo.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}